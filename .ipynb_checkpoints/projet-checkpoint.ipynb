{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zhufeng/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import zeros\n",
    "import os\n",
    "import re\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import itertools\n",
    "# import sys \n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"gbk\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful function for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyFunc(afilename):\n",
    "    #A key function used to sort the file list\n",
    "    m = re.search('(?<=Episode)\\d{2}', afilename)\n",
    "    return m.group(0)\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    #ref : https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "def get_frequent_word(tfidf, episod):\n",
    "    X = tfidf.transform([episod])\n",
    "    tfidf_table = X.todense().tolist()[0]\n",
    "    frequent_word = []\n",
    "    candidate_number = 20\n",
    "    for word in sorted(zip(tfidf_table,tfidf.get_feature_names()), reverse=True)[:candidate_number]:\n",
    "        frequent_word.append(word[1])\n",
    "    return frequent_word\n",
    "\n",
    "def make_sentence (untokenized):\n",
    "    #ref :https://github.com/thavelick/summarize/blob/master/summarize.py\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    normal_sentences = sent_detector.tokenize(untokenized) \n",
    "    working_sentences = [sentence.lower() for sentence in normal_sentences]\n",
    "    return working_sentences, normal_sentences\n",
    "\n",
    "\n",
    "def get_output_sentence(most_frequent_words, working_sentences, normal_sentences, num_phrase):\n",
    "    temp_sentences = []\n",
    "    output_sentences = []\n",
    "    #extract all sentence with high frequent word\n",
    "    for word in most_frequent_words:\n",
    "        for i in range(len(working_sentences)):\n",
    "            if (word in working_sentences[i] and normal_sentences[i] not in temp_sentences):\n",
    "                temp_sentences.append(normal_sentences[i]) \n",
    "    #class the sentences\n",
    "    counter = zeros(len(temp_sentences))\n",
    "    for i in range(len(temp_sentences)):\n",
    "        for word in most_frequent_words:\n",
    "            if word in temp_sentences[i]:\n",
    "                counter[i]+=tags_dict[word]\n",
    "        if len(temp_sentences[i])>15:\n",
    "            counter[i]=counter[i]-2\n",
    "    for sentence in sorted(zip(counter,temp_sentences), reverse=True)[:num_phrase]:\n",
    "        output_sentences.append(sentence[1])\n",
    "    return output_sentences\n",
    "\n",
    "def reorder_sentences(output_sentences, original):\n",
    "    ordered_output=[]\n",
    "    for sentence in original:\n",
    "        if sentence in output_sentences:\n",
    "            ordered_output.append(sentence)\n",
    "    return ordered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files and prepare the corpus for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_path = './sous_titre/'\n",
    "file_list = sorted(os.listdir(dir_path), key=keyFunc)\n",
    "collection=[] #document collection\n",
    "#sum=0\n",
    "error=[]\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path,\"rt\") as f:\n",
    "            lines=f.readlines()\n",
    "            episod=[]\n",
    "            for x in lines:\n",
    "                try:\n",
    "                    episod.append(x.split(\" \")[4])\n",
    "                except:\n",
    "                    #sum+=1\n",
    "                    error.append(x)\n",
    "                    continue\n",
    "        collection.append(episod)\n",
    "        \n",
    "untokenized_collection=[]\n",
    "        \n",
    "for episod in range(len(collection)):\n",
    "    untokenized_collection.append(untokenize(collection[episod]))\n",
    "\n",
    "\n",
    "\n",
    "#print (untokenized_collection[0])\n",
    "tags_list=[]\n",
    "for i in range(len(collection)):\n",
    "    tags_list.extend(nltk.pos_tag(collection[i]))\n",
    "\n",
    "tags_dict={}\n",
    "#we will give a weight for noun and others, preference on noun\n",
    "for tag in tags_list:\n",
    "    if (tag[1]=='NN' or tag[1]=='NNP' or tag[1]=='NNS') and tag[0] not in tags_dict:\n",
    "        tags_dict[tag[0]]=2\n",
    "    elif tag[0] not in tags_dict:\n",
    "        tags_dict[tag[0]]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', '.')\n"
     ]
    }
   ],
   "source": [
    "print (tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer le tf-idf and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There you go, Pad Thai, no peanuts. But does it have peanut oil? Uh, I'm not sure, everyone keep an eye on Howard in case he starts to swell up. Since it's not bee season, you can have my epinephrine. Are there any chopsticks? You don't need chopsticks, this is Thai food. Here we go. Thailand has had the fork since the latter half of the nineteenth century. Interestingly they don't actually put the fork in their mouth, they use it to put the food on a spoon which then goes into their mouth. Ask him for a napkin, I dare you. I'll get it. Do I look puffy? I feel puffy. Hey Leonard. Oh, hi Penny. Am I interrupting. No. You're not swelling, Howard. No, no, look at my fingers, they're like Vienna sausages. Sounds like you have company. They're not going anywhere. So, you're coming home from work. That's great. How was work. Well, you know, it's the Cheesecake Factory. People order cheesecake, and I bring it to them. So, you sort of act as a carbohydrate delivery system. Yeah, call it whatever you want, I get minimum wage. Yeah, anyways, I was wondering if you could help me out with something, I was... . Yes. Oh. Okay, great, I'm having some furniture delivered tomorrow, and I may not be here, so... . Oh! Hel ...hello! I'm sorry? Haven't you ever been told how beautiful you are in flawless Russian? No, I haven't. Get used to it. Yeah, I probably won't, but... Hey Sheldon. Hi. Hey Raj! Still not talking to me, huh? Don't take it personally, it's his pathology, he can't talk to women. He can't talk to attractive women, or in your case a cheesecake-scented Goddess! So, there's gonna be some furniture delivered? Yeah, yeah, if it gets here and I'm not here tomorrow could you just sign for it and have them put it in my apartment. Yeah, no problem. Great, here's my spare key. Thank you. Penny, wait. Yeah? Um, if you don't have any other plans, do you want to join us for Thai food and a Superman movie marathon? A marathon? Wow, how many Superman movies are there? You're kidding, right? Yeah, I do like the one where Lois Lane falls from the helicopter and Superman swooshes down and catches her, which one was that? One. You realize that scene was rife with scientific inaccuracy. Yes, I know, men can't fly. Oh no, let's assume that they can. Lois Lane is falling, accelerating at an initial rate of 32 feet per second per second. Superman swoops down to save her by reaching out two arms of steel. Miss Lane, who is now travelling at approximately 120 miles per hour, hits them, and is immediately sliced into three equal pieces. Unless, Superman matches her speed and decelerates. In what space, sir, in what space? She's two feet above the ground. Frankly, if he really loved her, he'd let her hit the pavement. It would be a more merciful death. Excuse me, your entire argument is predicated on the assumption that Superman's flight is a feat of strength. Are you listening to yourself, it is well established that Superman's flight is a feat of strength, it is an extension of his ability to leap tall buildings, an ability he derives from Earth's yellow Sun. Yeah, and you don't have a problem with that, how does he fly at night. Uh, a combination of the moon's solar reflection and the energy storage capacity of Kryptonian skin cells. I'm just going to go wash up. I have 26 hundred comic books in there, I challenge you to find a single reference to Kryptonian skin cells. Challenge accepted. We're locked out. Also, the pretty girl left. Okay, her apartment's on the fourth floor but the elevator's broken so you're going to have to oh, you're just going to be done, okay, cool, thanks. I guess we'll just bring it up ourselves. I hardly think so. Why not? Well, we don't have a dolly, or lifting belts, or any measurable upper body strength. We don't need strength, we're physicists. We are the intellectual descendents of Archimedes. Give me a fulcrum and a lever and I can move the Earth, it's just a matter... I don't have this... I don't have this I don't have this. Archimedes would be so proud. Do you have any ideas? Yes, but they all involve a green lantern and a power ring. Easy, easy Okay! Now we've got an inclined plane. The force required to lift is reduced by the sine of the angle of the stairs, call it thirty degrees, so about half. Exactly half. Exactly half. Let's push. Okay, see, it's moving, this is easy, all in the math. What's your formula for the corner. What? Oh, okay, uh, okay, yeah, no problem, just come up here and help me pull and turn. Ah, gravity, thou art a heartless bitch. You do understand that our efforts here will in no way increase the odds of you having sexual congress with this woman? Men do things for women without expecting sex. Yeah, those are men who just had sex. I'm doing this to be a good neighbor. In any case, there's no way it could lower the odds. Almost there, almost there, almost there. No we're not, no we're not, no we're not. Watch your fingers. Watch your fingers. Oh God, my fingers! You okay? No, it hurt... Great Caesar's Ghost, look at this place? So Penny's a little messy. A little messy? The Mandelbrot set of complex numbers is a little messy, this is chaos. Excuse me, explain to me an organizational system where a tray of flatware on a couch is valid. I'm just inferring that this is a couch, because the evidence suggests the coffee table's having a tiny garage sale. Did it ever occur to you that not everyone has the compulsive need to sort, organize and label the entire world around them? No. Well they don't. Hard as it may be for you to believe, most people don't sort their breakfast cereal numerically by fiber content. Excuse me, but I think we've both found that helpful at times. Come on, we should go. Hang on. What are you doing? Straightening up. Sheldon, this is not your home. This is not anyone's home, this is a swirling vortex of entropy. When the transvestite lived here, you didn't care how he kept the place. Because it was immaculate, I mean, you open that man's closet, it was left to right, evening gowns, cocktail dresses, then his police uniforms. What were you doing in his closet? I helped run some cable for a webcam. Hey guys. Oh, hey Penny, this just arrived, we just brought this up, just now. Great. Was it hard getting it up the stairs? No. No? No. No. Well, we'll get out of your hair. Oh, great, thank you again Penny, I just want you to know that, you don't have to live like this. I'm here for you. What's he talking about? It's a joke. I don't get it. Yeah, he didn't tell it right. Sheldon? Sheldon? Hello? Sheldon! Sssshhhh! Penny's sleeping. Are you insane, you can't just break into a woman's apartment in the middle of the night and clean. I had no choice. I couldn't sleep knowing that just outside my bedroom was our living room, and just outside our living room was that hallway, and immediately adjacent to that hallway was... this. Do you realize that if Penny wakes up, there is no reasonable explanation as to why we're here? I just gave you a reasonable explanation. No, no. You gave me an explanation, it's reasonableness will be determined by a jury of your peers. Don't be ridiculous. I have no peers. Sheldon, we have to get out of here. You might want to speak in a lower register. What? Evolution has made women sensitive to high pitched noises while they sleep, so that they'll be roused by a crying baby. If you want to avoid waking her, speak in a lower register. That's ridiculous. No, that's ridiculous. align \" left \" | Fine. I accept your premise, now please let's go. I am not leaving until I'm done. O-o-o-oh! If you have time to lean, you have time to clean. Oh, what the hell. Morning. Morning. I have to say, I slept splendidly. Granted, not long, but just deeply and well. I'm not surprised. A well known folk cure for insomnia is to break into your neighbor's apartment and clean. Sarcasm? You think? Granted, my methods may have been somewhat unorthodox, but I think the end result will be a measurable enhancement of Penny's quality of life. You know what, you've convinced me, maybe tonight we should sneak in and shampoo her carpet. You don't think that crosses a line? Yes! For God's sake, Sheldon, do I have to hold up a sarcasm sign every time I open my mouth. You have a sarcasm sign? No, I do not have a sarcasm sign. Do you want some cereal. I'm feeling so good today I'm going to choose from the low fiber end of the shelf. Hello, Honey Puffs. align= \" left \" | Son of a Bitch! Penny's up. You sick, geeky bastards! How did she know it was us? I may have left a suggested organizational schematic for her bedroom closet. align= \" left \" | Leonard! God, this is going to be bad. Goodbye, Honey Puffs, hello Big Bran. You came into my apartment last night when I was sleeping? Yes, but, only to clean. Really more to organize, you're not actually dirty, per se. Give me back my key. I'm very, very sorry. Do you understand how creepy this is. Oh, yes, we discussed it at length last night. In my apartment, while I was sleeping. And snoring. And that's probably just a sinus infection, but it could be sleep apnoea, you might want to see an otolaryngologist. It's a throat doctor. And what kind of doctor removes shoes from asses? Depending on the depth, that's either a proctologist or a general surgeon. Oh! God! Okay, look, no Penny, I think what you're feeling is perfectly valid, and maybe a little bit later today when you're feeling a little bit less, for lack of a better word, violated, maybe we could talk about this some more. Stay away from me. Sure, that's another way to go. Penny, Penny, just to clarify because there will be a discussion when you leave, is your objection solely to our presence in the apartment while you were sleeping, or do you also object to the imposition of a new organizational paradigm. Well that was a little non-responsive. You are going to march yourself over there right now and apologize. What's funny? That wasn't sarcasm? No. Wooh, boy, you are all over the place this morning. I have a masters and two PhD's, I should not have to do this. What? I am truly sorry for what happened last night, I take full responsibility. And I hope that it won't color your opinion of Leonard, who is not only a wonderful guy, but also, I hear, a gentle and thorough lover. I did what I could. Hey Raj. Hey, listen, I don't know if you heard about what happened last night with Leonard and Sheldon, but I'm really upset about it, I mean they just, they let themselves into my place, and then they cleaned it, I mean can you even believe that? How weird is that? Ooh, she's standing very close to me. Oh my, she does smell good. What is that, vanilla? You know, where I come from, someone comes into your house at night, you shoot, okay? And you don't shoot to wound. I mean, alright, my sister shot her husband, but it was an accident, they were drunk. What was I saying? She's so chatty. Maybe my parents are right. Maybe I'd be better off with an Indian girl. We'd have the same cultural background, and my wife would sing to my children the same lullabies my mother sang to me. It's obvious that they meant well, but I'm just, I'm having a really rough time, like I said, I broke up with my boyfriend, and it's just freaking me out. I mean, just because most of the men I've known in my life happen to be jerks, doesn't mean I should just assume Leonard and Sheldon are. Right? She asked me a question. I should probably nod. That's exactly what I thought. Thank you for listening. You're a doll. Oh-oh. Turn your pelvis. Grab a napkin, homie. You just got served. It's fine. You win. What's his problem? His imaginary girlfriend broke up with him. Been there. Hello. Sorry I'm late. But I was in the hallway, chatting up Penny. Really? You? Rajesh Koothrapali, spoke to Penny? Actually, I was less the chatter than the chattee. What did she say? Is she still mad at me? Well, she was upset at first, but, probably because her sister shot somebody. Then there was something about you and... then she hugged me. She hugged you? How did she hug you? Is that her perfume I smell? intoxicating, isn't it? Hi. Oh. What's going on? Um, here's the thing. Penny. Just as Oppenheimer came to regret his contributions to the first atomic bomb, so too I regret my participation in what was, at the very least, an error in judgment. The hallmark of the great human experiment is the willingness to recognize one's mistakes. Some mistakes, such as Madame Curie's discovery of Radium turned out to have great scientific potential even though she would later die a slow, painful death from radiation poisoning. Another example, from the field of ebola research... . Leonard. Yeah. We're okay. Six two inch dowels. Check. One package, Phillips head screws. Check. Guys, seriously, I grew up on a farm, okay, I rebuilt a tractor engine when I was like twelve, I think I can put together a cheap Swedish media center. No, please, we insist, it's the least we can do considering. Considering what? How great this place looks? align= \" left \" | Oh boy, I was afraid of this. What? These instructions are a pictographic representation of the least imaginative way to assemble these components. This right here is why Sweden has no space program. Well, uh, it looked pretty good in the store. It is an inefficient design, for example Penny has a flat screen TV, which means all the space behind it is wasted. We could put her stereo back there. And control it how? Run an infra-red repeater, photocell here, emitter here, easy peasy. align= \" left \" | Good point, how you gonna cool it? Hey guys, I got this. Hang on Penny. How about fans, here and here? Also inefficient, and might be loud. How about liquid coolant? Maybe a little aquarium pump here, run some quarter inch PVC... Guys, this is actually really simple. Hold on, honey, men at work. The PVC comes down here, maybe a little corrugated sheet metal as a radiator here. Oh, really, show me where we put a drip tray, a sluice and an overflow reservoir? And if water is involved we're going to have to ground the crap out of the thing. Guys, it's hot in here, I think I'll just take off all my clothes. Oh, I've got it. How about if we replace panels A, B and F and crossbar H with aircraft grade aluminum. Right, then the entire thing's one big heat sink. Perfect, Leonard, why don't you and Sheldon go down to the junk yard and pick up about six square meters of scrap aluminum, Raj and I will run down to my lab and get the oxy-acetaline torch. Meet back here in an hour? Done. Got it. Okay, this place does look pretty good.\n"
     ]
    }
   ],
   "source": [
    "dir_path = './sous_titre/'\n",
    "file_list = sorted(os.listdir(dir_path), key=keyFunc)\n",
    "collection=[] #document collection\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path,\"rt\") as f:\n",
    "            lines=f.readlines()\n",
    "            episod=[]\n",
    "            for x in lines:\n",
    "                episod.append(x.split(' ')[4])\n",
    "        collection.append(episod)\n",
    "        \n",
    "untokenized_collection=[]\n",
    "        \n",
    "for episod in range(len(collection)):\n",
    "    untokenized_collection.append(untokenize(collection[episod]))\n",
    "\n",
    "\n",
    "print (untokenized_collection[1])\n",
    "#print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared! You can make your summarize now!\n"
     ]
    }
   ],
   "source": [
    "oral_words = ['oh', 'yes', 'no', 'hi', 'okay', 'uh', 'okay', 'bye', 'sorry', 'well', 'think','know','going','yeah','hoo','aa'] #A compléter dans les tests\n",
    "fault_words = ['align','dr','choo','chka','mano','800','snipe','lane'] #There are some fault world in the corpus, which will pertube our summarization\n",
    "stop_words = stopwords.words('english') + list(punctuation) + oral_words + fault_words\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "tfidf.fit(untokenized_collection[episod] for episod in range(len(untokenized_collection)))\n",
    "print ('Model prepared! You can make your summarize now!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for changing the episod, just modify the index of untokenized_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Yeah, yeah, if it gets here and I'm not here tomorrow could you just sign for it and have them put it in my apartment.\", \"Excuse me, your entire argument is predicated on the assumption that Superman's flight is a feat of strength.\", \"Okay, her apartment's on the fourth floor but the elevator's broken so you're going to have to oh, you're just going to be done, okay, cool, thanks.\", \"Are you insane, you can't just break into a woman's apartment in the middle of the night and clean.\", \"A well known folk cure for insomnia is to break into your neighbor's apartment and clean.\", 'I may have left a suggested organizational schematic for her bedroom closet.', 'You came into my apartment last night when I was sleeping?', 'In my apartment, while I was sleeping.', 'Penny, Penny, just to clarify because there will be a discussion when you leave, is your objection solely to our presence in the apartment while you were sleeping, or do you also object to the imposition of a new organizational paradigm.', \"The hallmark of the great human experiment is the willingness to recognize one's mistakes.\"]\n"
     ]
    }
   ],
   "source": [
    "most_frequent_words = get_frequent_word(tfidf, untokenized_collection[1])\n",
    "working_sentences, normal_sentences = make_sentence (untokenized_collection[1])\n",
    "num_sentences = 10\n",
    "out=get_output_sentence(most_frequent_words, working_sentences, normal_sentences,num_sentences)\n",
    "print (reorder_sentences(out, normal_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
    "    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "#     return sentences.split(\" \")\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    words = _split_into_words(sentences)\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-N of two text collections of sentences.\n",
    "    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "    papers/rouge-working-note-v1.3.1.pdf\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentences: The sentences from the referene set\n",
    "      n: Size of ngram.  Defaults to 2.\n",
    "    Returns:\n",
    "      A tuple (f1, precision, recall) for ROUGE-N\n",
    "    Raises:\n",
    "      ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "#     print (evaluated_ngrams)\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    # Gets the overlapping ngrams between evaluated and reference\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1-score\": f1_score,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.6, 'Recall': 0.6, 'F1-score': 0.5999999950000001}\n",
      "{'Precision': 0.5, 'Recall': 0.5, 'F1-score': 0.4999999950000001}\n"
     ]
    }
   ],
   "source": [
    "h1=\"he is a good man.\"\n",
    "r1=\"she is a good woman.\"\n",
    "h1=[h1]\n",
    "r1=[r1]\n",
    "r_1=rouge_n(h1,r1,1)\n",
    "r_2=rouge_n(h1,r1,2)\n",
    "print(r_1)\n",
    "print(r_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _len_lcs(x, y):\n",
    "    \"\"\"\n",
    "    Returns the length of the Longest Common Subsequence between sequences x\n",
    "    and y.\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: sequence of words\n",
    "      y: sequence of words\n",
    "    Returns\n",
    "      integer: Length of LCS between x and y\n",
    "    \"\"\"\n",
    "    table = _lcs(x, y)\n",
    "    n, m = len(x), len(y)\n",
    "    return table[n, m]\n",
    "\n",
    "\n",
    "def _lcs(x, y):\n",
    "    \"\"\"\n",
    "    Computes the length of the longest common subsequence (lcs) between two\n",
    "    strings. The implementation below uses a DP programming algorithm and runs\n",
    "    in O(nm) time where n = len(x) and m = len(y).\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: collection of words\n",
    "      y: collection of words\n",
    "    Returns:\n",
    "      Table of dictionary of coord and len lcs\n",
    "    \"\"\"\n",
    "    n, m = len(x), len(y)\n",
    "    table = dict()\n",
    "    for i in range(n + 1):\n",
    "        for j in range(m + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                table[i, j] = 0\n",
    "            elif x[i - 1] == y[j - 1]:\n",
    "                table[i, j] = table[i - 1, j - 1] + 1\n",
    "            else:\n",
    "                table[i, j] = max(table[i - 1, j], table[i, j - 1])\n",
    "    return table\n",
    "\n",
    "\n",
    "def _recon_lcs(x, y):\n",
    "    \"\"\"\n",
    "    Returns the Longest Subsequence between x and y.\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: sequence of words\n",
    "      y: sequence of words\n",
    "    Returns:\n",
    "      sequence: LCS of x and y\n",
    "    \"\"\"\n",
    "    i, j = len(x), len(y)\n",
    "    table = _lcs(x, y)\n",
    "\n",
    "    def _recon(i, j):\n",
    "        \"\"\"private recon calculation\"\"\"\n",
    "        if i == 0 or j == 0:\n",
    "            return []\n",
    "        elif x[i - 1] == y[j - 1]:\n",
    "            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n",
    "        elif table[i - 1, j] > table[i, j - 1]:\n",
    "            return _recon(i - 1, j)\n",
    "        else:\n",
    "            return _recon(i, j - 1)\n",
    "\n",
    "    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n",
    "    return recon_tuple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _union_lcs(evaluated_sentences, reference_sentence, prev_union=None):\n",
    "    \"\"\"\n",
    "    Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n",
    "    subsequence between reference sentence ri and candidate summary C.\n",
    "    For example:\n",
    "    if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8\n",
    "    and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1\n",
    "    is \"w1 w2\" and the longest common subsequence of r_i and c2 is \"w1 w3 w5\".\n",
    "    The union longest common subsequence of r_i, c1, and c2 is \"w1 w2 w3 w5\"\n",
    "    and LCS_u(r_i, C) = 4/5.\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentence: One of the sentences in the reference summaries\n",
    "    Returns:\n",
    "      float: LCS_u(r_i, C)\n",
    "    ValueError:\n",
    "      Raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if prev_union is None:\n",
    "        prev_union = set()\n",
    "\n",
    "    if len(evaluated_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    lcs_union = prev_union\n",
    "    prev_count = len(prev_union)\n",
    "    reference_words = _split_into_words([reference_sentence])\n",
    "\n",
    "    combined_lcs_length = 0\n",
    "    for eval_s in evaluated_sentences:\n",
    "        evaluated_words = _split_into_words([eval_s])\n",
    "        lcs = set(_recon_lcs(reference_words, evaluated_words))\n",
    "        combined_lcs_length += len(lcs)\n",
    "        lcs_union = lcs_union.union(lcs)\n",
    "\n",
    "    new_lcs_count = len(lcs_union) - prev_count\n",
    "    return new_lcs_count, lcs_union\n",
    "\n",
    "\n",
    "def rouge_l_summary_level(evaluated_sentences, reference_sentences):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-L (summary level) of two text collections of sentences.\n",
    "    http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n",
    "    rouge-working-note-v1.3.1.pdf\n",
    "    Calculated according to:\n",
    "    R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n",
    "    P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n",
    "    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n",
    "    where:\n",
    "    SUM(i,u) = SUM from i through u\n",
    "    u = number of sentences in reference summary\n",
    "    C = Candidate summary made up of v sentences\n",
    "    m = number of words in reference summary\n",
    "    n = number of words in candidate summary\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentence: One of the sentences in the reference summaries\n",
    "    Returns:\n",
    "      A float: F_lcs\n",
    "    Raises:\n",
    "      ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    # total number of words in reference sentences\n",
    "    m = len(set(_split_into_words(reference_sentences)))\n",
    "\n",
    "    # total number of words in evaluated sentences\n",
    "    n = len(set(_split_into_words(evaluated_sentences)))\n",
    "\n",
    "    # print(\"m,n %d %d\" % (m, n))\n",
    "    union_lcs_sum_across_all_references = 0\n",
    "    union = set()\n",
    "    for ref_s in reference_sentences:\n",
    "        lcs_count, union = _union_lcs(evaluated_sentences,\n",
    "                                      ref_s,\n",
    "                                      prev_union=union)\n",
    "        union_lcs_sum_across_all_references += lcs_count\n",
    "\n",
    "    llcs = union_lcs_sum_across_all_references\n",
    "    r_lcs = llcs / m\n",
    "    p_lcs = llcs / n\n",
    "    beta = p_lcs / (r_lcs + 1e-12)\n",
    "    num = (1 + (beta**2)) * r_lcs * p_lcs\n",
    "    denom = r_lcs + ((beta**2) * p_lcs)\n",
    "    f_lcs = num / (denom + 1e-12)\n",
    "    return {\"Precision\": p_lcs, \"Recall\": r_lcs, \"F1-score\": f_lcs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.6, 'Recall': 0.6, 'F1-score': 0.5999999999994999}\n"
     ]
    }
   ],
   "source": [
    "r_l=rouge_l_summary_level(h1,r1)\n",
    "print(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits.\", \"That doesn't seem right.\", \"We don't mean to interrupt, we live across the hall.\", 'Well, uh, oh, welcome to the building.', 'I think we should be good neighbors, invite her over, make her feel welcome.', 'Who hasn\\'t seen this differential below \" here I sit broken hearted? \"', \"Okay, let's see, what else, oh, I'm a vegetarian, oh, except for fish, and the occasional steak, I love steak.\", 'Well, if that was a movie I would go see it.', \"I'm not going to engage in hypotheticals here, I'm just trying to be a good neighbor.\", 'Wait till you see this.', 'Technically that would be coitus interruptus.', 'Sure, you can ask me a favour, I would do you a favour for you.', \"We drive half way across town to retrieve a television set from the aforementioned woman's ex-boyfriend.\", 'Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building.', 'How the hell did you get in the building?']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r_untokenized_collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-965be8174a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mreference_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr_untokenized_collection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r_untokenized_collection' is not defined"
     ]
    }
   ],
   "source": [
    "test=reorder_sentences(out,normal_sentences)\n",
    "print(test)\n",
    "\n",
    "reference_text=r_untokenized_collection[0]\n",
    "t1,t2=make_sentence(reference_text)\n",
    "\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"\"\n",
    "for i in t1:\n",
    "    s+=i\n",
    "s=[s]\n",
    "\n",
    "s1=\"\"\n",
    "for i in test:\n",
    "    s1+=i\n",
    "\n",
    "s1=[s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyp, ref in zip(s, s1):\n",
    "        hyp = [\" \".join(_.split()) for _ in hyp.split(\".\") if len(_) > 0]\n",
    "        ref = [\" \".join(_.split()) for _ in ref.split(\".\") if len(_) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Recall': 0.14193548387096774, u'F1-score': 0.1605839366921521, u'Precision': 0.18487394957983194}\n",
      "{u'Recall': 0.009615384615384616, u'F1-score': 0.010554084757418382, u'Precision': 0.011695906432748537}\n",
      "{u'Recall': 0.13548387096774195, u'F1-score': 0.14825308918355073, u'Precision': 0.17647058823529413}\n"
     ]
    }
   ],
   "source": [
    "print(rouge_n(hyp,ref,1))\n",
    "print(rouge_n(hyp,ref,2))\n",
    "print(rouge_l_summary_level(hyp,ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "呜啦啦啦火车笛，做projet很忙滴。。大家唱起来！俗话说的好，彩蛋最后找，要是没找到。。。。。。。。。。。。。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "说明你小子根本没仔细看劳资辛辛苦苦写的代码！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
