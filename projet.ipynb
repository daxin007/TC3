{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import zeros\n",
    "import os\n",
    "import re\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import itertools\n",
    "# import sys \n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful function for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyFunc(afilename):\n",
    "    #A key function used to sort the file list\n",
    "    m = re.search('(?<=Episode)\\d{2}', afilename)\n",
    "    return m.group(0)\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    #ref : https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "def get_frequent_word(tfidf, episod):\n",
    "    X = tfidf.transform([episod])\n",
    "    tfidf_table = X.todense().tolist()[0]\n",
    "    frequent_word = []\n",
    "    candidate_number = 20\n",
    "    for word in sorted(zip(tfidf_table,tfidf.get_feature_names()), reverse=True)[:candidate_number]:\n",
    "        frequent_word.append(word[1])\n",
    "    return frequent_word\n",
    "\n",
    "def make_sentence (untokenized):\n",
    "    #ref :https://github.com/thavelick/summarize/blob/master/summarize.py\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    normal_sentences = sent_detector.tokenize(untokenized) \n",
    "    working_sentences = [sentence.lower() for sentence in normal_sentences]\n",
    "    return working_sentences, normal_sentences\n",
    "\n",
    "\n",
    "def get_output_sentence(most_frequent_words, working_sentences, normal_sentences, num_phrase):\n",
    "    temp_sentences = []\n",
    "    output_sentences = []\n",
    "    for word in most_frequent_words:\n",
    "        for i in range(len(working_sentences)):\n",
    "            if (word in working_sentences[i] and normal_sentences[i] not in temp_sentences):\n",
    "                temp_sentences.append(normal_sentences[i]) \n",
    "    counter = zeros(len(temp_sentences))\n",
    "    for i in range(len(temp_sentences)):\n",
    "        for word in most_frequent_words:\n",
    "            if word in temp_sentences[i]:\n",
    "                counter[i]+=1        \n",
    "    for sentence in sorted(zip(counter,temp_sentences), reverse=True)[:num_phrase]:\n",
    "        output_sentences.append(sentence[1])\n",
    "    return output_sentences\n",
    "\n",
    "def reorder_sentences(output_sentences, original):\n",
    "    ordered_output=[]\n",
    "    for sentence in original:\n",
    "        if sentence in output_sentences:\n",
    "            ordered_output.append(sentence)\n",
    "    return ordered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files and prepare the corpus for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc3 in position 56: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-560902476529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mepisod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mepisod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc3 in position 56: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "dir_path = './sous_titre/'\n",
    "file_list = sorted(os.listdir(dir_path), key=keyFunc)\n",
    "collection=[] #document collection\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path,\"rt\") as f:\n",
    "            lines=f.readlines()\n",
    "            episod=[]\n",
    "            for x in lines:\n",
    "                episod.append(x.split(' ')[4])\n",
    "        collection.append(episod)\n",
    "        \n",
    "untokenized_collection=[]\n",
    "        \n",
    "for episod in range(len(collection)):\n",
    "    untokenized_collection.append(untokenize(collection[episod]))\n",
    "\n",
    "\n",
    "print (untokenized_collection[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer le tf-idf and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './summary/'\n",
    "f_list = sorted(os.listdir(dir_path))\n",
    "file_list=[]\n",
    "file_list.append(f_list[0])\n",
    "file_list.extend(f_list[9:])\n",
    "file_list.extend(f_list[1:9])\n",
    "r_collection=[] #document collection\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path,\"rt\") as f:\n",
    "            lines=f.readlines()\n",
    "            episod=[]\n",
    "            for x in lines:\n",
    "                episod.append(x)\n",
    "        r_collection.append(episod)\n",
    "        \n",
    "r_untokenized_collection=[]\n",
    "        \n",
    "for episod in range(len(r_collection)):\n",
    "    r_untokenized_collection.append(untokenize(r_collection[episod]))\n",
    "\n",
    "\n",
    "print (r_untokenized_collection[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oral_words = ['oh', 'yes', 'no', 'hi', 'okay', 'uh', 'okay', 'bye', 'sorry', 'well', 'think','know','going','yeah'] #A compl√©ter dans les tests\n",
    "stop_words = stopwords.words('english') + list(punctuation) + oral_words\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "tfidf.fit(untokenized_collection[episod] for episod in range(len(untokenized_collection)))\n",
    "print ('Model prepared! You can make your summarize now!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for changing the episod, just modify the index of untokenized_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "most_frequent_words = get_frequent_word(tfidf, untokenized_collection[0])\n",
    "working_sentences, normal_sentences = make_sentence (untokenized_collection[0])\n",
    "num_sentences = 15\n",
    "out=get_output_sentence(most_frequent_words, working_sentences, normal_sentences,num_sentences)\n",
    "print (reorder_sentences(out, normal_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
    "    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "#     return sentences.split(\" \")\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    words = _split_into_words(sentences)\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-N of two text collections of sentences.\n",
    "    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "    papers/rouge-working-note-v1.3.1.pdf\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentences: The sentences from the referene set\n",
    "      n: Size of ngram.  Defaults to 2.\n",
    "    Returns:\n",
    "      A tuple (f1, precision, recall) for ROUGE-N\n",
    "    Raises:\n",
    "      ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "#     print (evaluated_ngrams)\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    # Gets the overlapping ngrams between evaluated and reference\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1-score\": f1_score,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1=\"he is a good man.\"\n",
    "r1=\"she is a good woman.\"\n",
    "h1=[h1]\n",
    "r1=[r1]\n",
    "r_1=rouge_n(h1,r1,1)\n",
    "r_2=rouge_n(h1,r1,2)\n",
    "print(r_1)\n",
    "print(r_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _len_lcs(x, y):\n",
    "    \"\"\"\n",
    "    Returns the length of the Longest Common Subsequence between sequences x\n",
    "    and y.\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: sequence of words\n",
    "      y: sequence of words\n",
    "    Returns\n",
    "      integer: Length of LCS between x and y\n",
    "    \"\"\"\n",
    "    table = _lcs(x, y)\n",
    "    n, m = len(x), len(y)\n",
    "    return table[n, m]\n",
    "\n",
    "\n",
    "def _lcs(x, y):\n",
    "    \"\"\"\n",
    "    Computes the length of the longest common subsequence (lcs) between two\n",
    "    strings. The implementation below uses a DP programming algorithm and runs\n",
    "    in O(nm) time where n = len(x) and m = len(y).\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: collection of words\n",
    "      y: collection of words\n",
    "    Returns:\n",
    "      Table of dictionary of coord and len lcs\n",
    "    \"\"\"\n",
    "    n, m = len(x), len(y)\n",
    "    table = dict()\n",
    "    for i in range(n + 1):\n",
    "        for j in range(m + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                table[i, j] = 0\n",
    "            elif x[i - 1] == y[j - 1]:\n",
    "                table[i, j] = table[i - 1, j - 1] + 1\n",
    "            else:\n",
    "                table[i, j] = max(table[i - 1, j], table[i, j - 1])\n",
    "    return table\n",
    "\n",
    "\n",
    "def _recon_lcs(x, y):\n",
    "    \"\"\"\n",
    "    Returns the Longest Subsequence between x and y.\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: sequence of words\n",
    "      y: sequence of words\n",
    "    Returns:\n",
    "      sequence: LCS of x and y\n",
    "    \"\"\"\n",
    "    i, j = len(x), len(y)\n",
    "    table = _lcs(x, y)\n",
    "\n",
    "    def _recon(i, j):\n",
    "        \"\"\"private recon calculation\"\"\"\n",
    "        if i == 0 or j == 0:\n",
    "            return []\n",
    "        elif x[i - 1] == y[j - 1]:\n",
    "            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n",
    "        elif table[i - 1, j] > table[i, j - 1]:\n",
    "            return _recon(i - 1, j)\n",
    "        else:\n",
    "            return _recon(i, j - 1)\n",
    "\n",
    "    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n",
    "    return recon_tuple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _union_lcs(evaluated_sentences, reference_sentence, prev_union=None):\n",
    "    \"\"\"\n",
    "    Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n",
    "    subsequence between reference sentence ri and candidate summary C.\n",
    "    For example:\n",
    "    if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8\n",
    "    and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1\n",
    "    is \"w1 w2\" and the longest common subsequence of r_i and c2 is \"w1 w3 w5\".\n",
    "    The union longest common subsequence of r_i, c1, and c2 is \"w1 w2 w3 w5\"\n",
    "    and LCS_u(r_i, C) = 4/5.\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentence: One of the sentences in the reference summaries\n",
    "    Returns:\n",
    "      float: LCS_u(r_i, C)\n",
    "    ValueError:\n",
    "      Raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if prev_union is None:\n",
    "        prev_union = set()\n",
    "\n",
    "    if len(evaluated_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    lcs_union = prev_union\n",
    "    prev_count = len(prev_union)\n",
    "    reference_words = _split_into_words([reference_sentence])\n",
    "\n",
    "    combined_lcs_length = 0\n",
    "    for eval_s in evaluated_sentences:\n",
    "        evaluated_words = _split_into_words([eval_s])\n",
    "        lcs = set(_recon_lcs(reference_words, evaluated_words))\n",
    "        combined_lcs_length += len(lcs)\n",
    "        lcs_union = lcs_union.union(lcs)\n",
    "\n",
    "    new_lcs_count = len(lcs_union) - prev_count\n",
    "    return new_lcs_count, lcs_union\n",
    "\n",
    "\n",
    "def rouge_l_summary_level(evaluated_sentences, reference_sentences):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-L (summary level) of two text collections of sentences.\n",
    "    http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n",
    "    rouge-working-note-v1.3.1.pdf\n",
    "    Calculated according to:\n",
    "    R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n",
    "    P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n",
    "    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n",
    "    where:\n",
    "    SUM(i,u) = SUM from i through u\n",
    "    u = number of sentences in reference summary\n",
    "    C = Candidate summary made up of v sentences\n",
    "    m = number of words in reference summary\n",
    "    n = number of words in candidate summary\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentence: One of the sentences in the reference summaries\n",
    "    Returns:\n",
    "      A float: F_lcs\n",
    "    Raises:\n",
    "      ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    # total number of words in reference sentences\n",
    "    m = len(set(_split_into_words(reference_sentences)))\n",
    "\n",
    "    # total number of words in evaluated sentences\n",
    "    n = len(set(_split_into_words(evaluated_sentences)))\n",
    "\n",
    "    # print(\"m,n %d %d\" % (m, n))\n",
    "    union_lcs_sum_across_all_references = 0\n",
    "    union = set()\n",
    "    for ref_s in reference_sentences:\n",
    "        lcs_count, union = _union_lcs(evaluated_sentences,\n",
    "                                      ref_s,\n",
    "                                      prev_union=union)\n",
    "        union_lcs_sum_across_all_references += lcs_count\n",
    "\n",
    "    llcs = union_lcs_sum_across_all_references\n",
    "    r_lcs = llcs / m\n",
    "    p_lcs = llcs / n\n",
    "    beta = p_lcs / (r_lcs + 1e-12)\n",
    "    num = (1 + (beta**2)) * r_lcs * p_lcs\n",
    "    denom = r_lcs + ((beta**2) * p_lcs)\n",
    "    f_lcs = num / (denom + 1e-12)\n",
    "    return {\"Precision\": p_lcs, \"Recall\": r_lcs, \"F1-score\": f_lcs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l=rouge_l_summary_level(h1,r1)\n",
    "print(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=reorder_sentences(out,normal_sentences)\n",
    "print(test)\n",
    "\n",
    "reference_text=r_untokenized_collection[0]\n",
    "t1,t2=make_sentence(reference_text)\n",
    "\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"\"\n",
    "for i in t1:\n",
    "    s+=i\n",
    "s=[s]\n",
    "\n",
    "s1=\"\"\n",
    "for i in test:\n",
    "    s1+=i\n",
    "\n",
    "s1=[s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyp, ref in zip(s, s1):\n",
    "        hyp = [\" \".join(_.split()) for _ in hyp.split(\".\") if len(_) > 0]\n",
    "        ref = [\" \".join(_.split()) for _ in ref.split(\".\") if len(_) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rouge_n(hyp,ref,1))\n",
    "print(rouge_n(hyp,ref,2))\n",
    "print(rouge_l_summary_level(hyp,ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂëúÂï¶Âï¶Âï¶ÁÅ´ËΩ¶Á¨õÔºåÂÅöprojetÂæàÂøôÊª¥„ÄÇ„ÄÇÂ§ßÂÆ∂Âî±Ëµ∑Êù•ÔºÅ‰øóËØùËØ¥ÁöÑÂ•ΩÔºåÂΩ©ËõãÊúÄÂêéÊâæÔºåË¶ÅÊòØÊ≤°ÊâæÂà∞„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ„ÄÇ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ËØ¥Êòé‰Ω†Â∞èÂ≠êÊ†πÊú¨Ê≤°‰ªîÁªÜÁúãÂä≥ËµÑËæõËæõËã¶Ëã¶ÂÜôÁöÑ‰ª£Á†ÅÔºÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
