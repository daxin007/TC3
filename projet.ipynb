{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import zeros\n",
    "import os\n",
    "import re\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import itertools\n",
    "# import sys \n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"gbk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some useful function for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyFunc(afilename):\n",
    "    #A key function used to sort the file list\n",
    "    m = re.search('(?<=Episode)\\d{2}', afilename)\n",
    "    return m.group(0)\n",
    "\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    #ref : https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .', '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "        \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "def get_frequent_word(tfidf, episod):\n",
    "    X = tfidf.transform([episod])\n",
    "    tfidf_table = X.todense().tolist()[0]\n",
    "    frequent_word = []\n",
    "    candidate_number = 20\n",
    "    for word in sorted(zip(tfidf_table,tfidf.get_feature_names()), reverse=True)[:candidate_number]:\n",
    "        frequent_word.append(word[1])\n",
    "    return frequent_word\n",
    "\n",
    "def make_sentence (untokenized):\n",
    "    #ref :https://github.com/thavelick/summarize/blob/master/summarize.py\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    normal_sentences = sent_detector.tokenize(untokenized) \n",
    "    working_sentences = [sentence.lower() for sentence in normal_sentences]\n",
    "    return working_sentences, normal_sentences\n",
    "\n",
    "\n",
    "def get_output_sentence(most_frequent_words, working_sentences, normal_sentences, num_phrase):\n",
    "    temp_sentences = []\n",
    "    output_sentences = []\n",
    "    for word in most_frequent_words:\n",
    "        for i in range(len(working_sentences)):\n",
    "            if (word in working_sentences[i] and normal_sentences[i] not in temp_sentences):\n",
    "                temp_sentences.append(normal_sentences[i]) \n",
    "    counter = zeros(len(temp_sentences))\n",
    "    for i in range(len(temp_sentences)):\n",
    "        for word in most_frequent_words:\n",
    "            if word in temp_sentences[i]:\n",
    "                counter[i]+=1        \n",
    "    for sentence in sorted(zip(counter,temp_sentences), reverse=True)[:num_phrase]:\n",
    "        output_sentences.append(sentence[1])\n",
    "    return output_sentences\n",
    "\n",
    "def reorder_sentences(output_sentences, original):\n",
    "    ordered_output=[]\n",
    "    for sentence in original:\n",
    "        if sentence in output_sentences:\n",
    "            ordered_output.append(sentence)\n",
    "    return ordered_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files and prepare the corpus for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So if a photon is directed through a plane with two slits in it and either slit is observed, it will not go through both slits. If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits. Agreed, what's your point? There's no point, I just think it's a good idea for a tee-shirt. Excuse me? Hang on. One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is... move your finger... phylum, which makes fourteen across Port-au-Prince. See, Papa Doc's capital idea, that's Port-au-Prince. Haiti. Can I help you? Yes. Um, is this the High IQ sperm bank? If you have to ask, maybe you shouldn't be here. I think this is the place. Fill these out. Thank-you. We'll be right back. Oh, take your time. I'll just finish my crossword puzzle. Oh, wait. Leonard, I don't think I can do this. What, are you kidding? You're a semi-pro. No. We are committing genetic fraud. There's no guarantee that our sperm is going to generate high IQ offspring, think about that. I have a sister with the same basic DNA mix who hostesses at Fuddruckers. Sheldon, this was your idea. A little extra money to get fractional T1 bandwidth in the apartment. I know, and I do yearn for faster downloads, but there's some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn't know if he should use an integral or a differential to solve the area under a curve. I'm sure she'll still love him. I wouldn't. Well, what do you want to do? I want to leave. Okay. What's the protocol for leaving? I don't know, I've never reneged on a proffer of sperm before. Let's try just walking out. Okay. Bye. Bye-bye See you. Are you still mad about the sperm bank? No. You want to hear an interesting thing about stairs? Not really. If the height of a single step is off by as little as two millimetres, most people will trip. I don't care. Two millimetres? That doesn't seem right. No, it's true, I did a series of experiments when I was twelve, my father broke his clavicle. Is that why they sent you to boarding school? No, that was the result of my work with lasers. New neighbor? Evidently. Significant improvement over the old neighbor. Two hundred pound transvestite with a skin condition, yes she is. Oh, hi! Hi. Hi. Hi. Hi. Hi? We don't mean to interrupt, we live across the hall. Oh, that's nice. Oh... uh... no... we don't live together... um... we live together but in separate, heterosexual bedrooms. Oh, okay, well, guess I'm your new neighbor, Penny. Leonard, Sheldon. Hi. Hi. Hi. Hi. Hi. Well, uh, oh, welcome to the building. Thank you, maybe we can have coffee sometime. Oh, great. Great. Great. Great. Well, bye. Bye. Bye. Bye. Should we have invited her for lunch? No. We're going to start Season Two of \" Battlestar Galactica \". We already watched the Season Two DVDs. Not with commentary. I think we should be good neighbors, invite her over, make her feel welcome. We never invited Louis-slash-Louise over. Well, then that was wrong of us. We need to widen our circle. I have a very wide circle. I have 212 friends on myspace. Yes, and you've never met one of them. That's the beauty of it. I'm going to invite her over. We'll have a nice meal and chat. Chat? We don't chat. At least not offline. Well it's not difficult, you just listen to what she says, and then you say something appropriate in response. To what end? Hi. Again. Hi. Hi. Hi. Hi. Anyway, um. We brought home Indian food. And, um. I know that moving can be stressful, and I find that when I'm undergoing stress, that good food and company can have a comforting effect. Also, curry is a natural laxative, and I don't have to tell you that, uh, a clean colon is just one less thing to worry about. Leonard, I'm not expert here but I believe in the context of a luncheon invitation, you might want to skip the reference to bowel movements. Oh, you're inviting me over to eat? Uh, yes. Oh, that's so nice, I'd love to. Great. So, what do you guys do for fun around here? Well, today we tried masturbating for money. Okay, well, make yourself at home. Okay, thankyou. You're very welcome. This looks like some serious stuff, Leonard, did you do this? Actually that's my work. Wow. Yeah, well, it's just some quantum mechanics, with a little string theory doodling around the edges. That part there, that's just a joke, it's a spoof of the Bourne-Oppenheimer approximation. So you're like, one of those, beautiful mind genius guys. Yeah. This is really impressive. I have a board. If you like boards, this is my board. Holy smokes. If by holy smokes you mean a derivative restatement of the kind of stuff you can find scribbled on the wall of any men's room at MIT, sure. What? Oh, come on. Who hasn't seen this differential below \" here I sit broken hearted? \" At least I didn't have to invent twenty-six dimensions just to make the math come out. I didn't invent them, they're there. In what universe? In all of them, that is the point. Uh, do you guys mind if I start? Um, Penny... that's where I sit. So, sit next to me. ... No, I sit there. What's the difference? \" What's the difference \"? Here we go. In the winter that seat is close enough to the radiator to remain warm, and yet not so close as to cause perspiration. In the summer it's directly in the path of a cross breeze created by open windows there, and there. It faces the television at an angle that is neither direct, thus discouraging conversation, nor so far wide to create a parallax distortion. I could go on, but I think I've made my point. Do you want me to move? Well- Just sit somewhere else. Fine. Sheldon, sit! Aaah! Well, this is nice. We don't have a lot of company over. That's not true. Koothrapali and Wolowitz come over all the time. Yes, I know, but... Tuesday night we played Klingon boggle until one in the morning. Yes, I remember. I resent you saying we don't have company. I'm sorry. That is an antisocial implication. I said I'm sorry. So, Klingon boggle? Yeah, it's like regular boggle, but in Klingon. That's probably enough about us, tell us about you. Um, me, okay, I'm Sagittarius, which probably tells you way more than you need to know. Yes, it tells us that you participate in the mass cultural delusion that the Sun's apparent position relative to arbitrarily defined constellations and the time of your birth somehow effects your personality. Participate in the what? I think what Sheldon's trying to say, is that Sagittarius wouldn't have been our first guess. Oh, yeah, a lot of people think I'm a water sign. Okay, let's see, what else, oh, I'm a vegetarian, oh, except for fish, and the occasional steak, I love steak. That's interesting. Leonard can't process corn. Wu-uh, do you have some sort of a job? Oh, yeah, I'm a waitress at the Cheesecake Factory. Oh, okay. I love cheesecake. You're lactose intolerant. I don't eat it, I just think it's a good idea. Oh, anyways, I'm also writing a screenplay. It's about this sensitive girl who comes to L.A. from Lincoln Nebraska to be an actress, and winds up a waitress at the Cheesecake Factory. So it's based on your life? No, I'm from Omaha. Well, if that was a movie I would go see it. I know, right? Okay, let's see, what else? Um, that's about it. That's the story of Penny. Well it sounds wonderful. It was. Until I fell in love with a jerk. What's happening. I don't know. Oh God, you know, four years I lived with him, four years, that's like as long as High School. It took you four years to get through High School? Don't. I just, I can't believe I trusted him. Should I say something? I feel like I should say something. You? No, you'll only make it worse. You want to know the most pathetic part? Even though I hate his lying, cheating guts, I still love him. Is that crazy? Yes. No, it's not crazy it's, uh, uh, it's a paradox. And paradoxes are part of nature, think about light. Now if you look at Huygens, light is a wave, as confirmed by the double slit experiments, but then, along comes Albert Einstein and discovers that light behaves like particles too. Well, I didn't make it worse. Oh, I'm so sorry, I'm such a mess, and on top of everything else I'm all gross from moving and my stupid shower doesn't even work. Our shower works. Really? Would it be totally weird if I used it? Yes. No. No? No. No. It's right down the hall. Thanks. You guys are really sweet. Well this is an interesting development. How so? It has been some time since we've had a woman take her clothes off in our apartment. That's not true, remember at Thanksgiving my grandmother with Alzheimer's had that episode. Point taken. It has been some time since we've had a woman take her clothes off after which we didn't want to rip our eyes out. The worst part was watching her carve that turkey. So, what exactly are you trying to accomplish here? Excuse me? That woman in there's not going to have sex with you. Well I'm not trying to have sex with her. Oh, good. Then you won't be disappointed. What makes you think she wouldn't have sex with me, I'm a male and she's a female? Yes, but not of the same species. I'm not going to engage in hypotheticals here, I'm just trying to be a good neighbor. Oh, of course. That's not to say that if a carnal relationship were to develop that I wouldn't participate. However briefly. Do you think this possibility will be helped or hindered when she discovers your Luke Skywalker no-more-tears shampoo? It's Darth Vader shampoo. Luke Skywalker's the conditioner. Wait till you see this. It's fantastic. Unbelievable. See what? It's a Stephen Hawking lecture from MIT in 1974. This is not a good time. It's before he became a creepy computer voice That's great, you guys have to go. Why? It's just not a good time. Leonard has a lady over. Yeah, right, your grandmother back in town? No. And she's not a lady, she's just a new neighbor. Hang on, there really is a lady here? Uh-huh. And you want us out because you're anticipating coitus? I'm not anticipating coitus. So she's available for coitus? Can we please stop saying coitus? Technically that would be coitus interruptus. Hey, is there a trick to getting it to switch from tub to shower? Oh. Hi, sorry. Hello! Enchante, Madamoiselle. Howard Wolowitz, Cal-Tech department of Applied Physics. You may be familiar with some of my work, it's currently orbiting Jupiter's largest moon taking high-resolution digital photographs. Penny. I work at the Cheesecake Factory. Come on, I'll show you the trick with the shower. Bon douche. I'm sorry? It's French for good shower. It's a sentiment I can express in six languages. Save it for your blog, Howard. See-ka-tong-guay-jow. Uh, there it goes, it sticks, I'm sorry. Okay. Thanks. You're welcome, oh, you're going to step right, okay, I'll... . Hey, Leonard? The hair products are Sheldon's. Um, okay. Can I ask you a favour. A favour? Sure, you can ask me a favour, I would do you a favour for you. It's okay if you say no. Oh, I'll probably say yes. It's just not the kind of thing you ask a guy you've just met. Wow. I really think we should examine the chain of causality here. Must we? Event A. A beautiful woman stands naked in our shower. Event B. We drive half way across town to retrieve a television set from the aforementioned woman's ex-boyfriend. Query, on what plane of existence is there even a semi-rational link between these events? She asked me to do her a favour, Sheldon. Ah, yes, well that may be the proximal cause of our journey, but we both know it only exists in contradistinction to the higher level distal cause. Which is? You think with your penis. That's a biological impossibility and you didn't have to come. Oh, right, yes, I could have stayed behind and watched Wolowitz try to hit on Penny in Russian, Arabic and Farsi. Why can't she get her own TV. Come on, you know how it is with break-ups. No, I don't. And neither do you. Wuh, I, I broke up with Joyce Kim. You did not break up with Joyce Kim, she defected to North Korea. To mend her broken heart. This situation is much less complicated. There's some kind of dispute between Penny and her ex-boyfriend as to who gets custody of the TV. She just wanted to avoid having a scene with him. So we get to have a scene with him? No, Sheldon, there's not going to be a scene. There's two of us and one of him. Leonard, the two of us can't even carry a TV. So, you guys work with Leonard and Sheldon at the University? Uh, I'm sorry, do you speak English? Oh, he speaks English, he just can't speak to women. Really, why? He's kind of a nerd. Juice box? I'll do the talking. Yeah. Hi, I'm Leonard, this is Sheldon. Hello. What did I just... . Uh, we're here to pick up Penny's TV. Get lost. Okay, thanks for your time. We're not going to give up just like that. Leonard, the TV is in the building, we've been denied access to the building, ergo we are done. Excuse me, if I were to give up at the first little hitch I never would have been able to identify the fingerprints of string theory in the aftermath of the big bang. My apologies. What's your plan. It's just a privilege to watch your mind at work. Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building. What do you think their combined IQ is? Just grab the door. This is it. I'll do the talking. Good thinking, I'll just be the muscle. Yeah? I'm Leonard, this is Sheldon. From the intercom. How the hell did you get in the building? Oh. We're scientists. Tell him about our IQ. Leonard. What? My mom bought me those pants. I'm sorry. You're going to have to call her. Sheldon, I'm so sorry I dragged you through this. It's okay. It wasn't my first pantsing, and it won't be my last. And you were right about my motives, I was hoping to establish a relationship with Penny that might have some day led to sex. Well you got me out of my pants. Anyway, I've learned my lesson. She's out of my league, I'm done with her, I've got my work, one day I'll win the Nobel Prize and then I'll die alone. Don't think like that, you're not going to die alone. Thank you Sheldon, you're a good friend. And you're certainly not going to win a Nobel Prize. This is one of my favorite places to kick back after a quest, they have a great house ale. Wow, cool tiger. Yeah, I've had him since level ten. His name is Buttons. Anyway, if you had your own game character we could hang out, maybe go on a quest. Uh, sounds interesting. So you'll think about it? Oh, I don't think I'll be able to stop thinking about it. Smooth. We're home. Oh, my God, what happened? Well, your ex-boyfriend sends his regards and I think the rest is fairly self-explanatory. I'm so sorry, I really thought if you guys went instead of me he wouldn't be such an ass. No, it was a valid hypothesis. That was a valid hypothesis? What is happening to you? Really, thank you so much for going and trying you're, uh, you're so terrific. Why don't you put some clothes on, I'll get my purse and dinner is on me, okay? Really? Great. Thank you. You're not done with her, are you? Our babies will be smart and beautiful. Not to mention imaginary. Is Thai food okay with you Penny? Sure. We can't have Thai food, we had Indian for lunch. So? They're both curry based cuisines. So? They would be gastronomically redundant. I can see we're going to have to spell out everything for this girl. Any ideas Raj? Turn left on Lake Street and head up to Colorado. I know a wonderful little sushi bar that has karaoke. That sounds like fun. Baby, baby don't get hooked on me. Uh, baby, baby don't get hooked on me. I don't know what your odds are in the world as a whole, but as far as the population of this car goes, you're a veritable Mack Daddy.\n"
     ]
    }
   ],
   "source": [
    "dir_path = './sous_titre/'\n",
    "file_list = sorted(os.listdir(dir_path), key=keyFunc)\n",
    "collection=[] #document collection\n",
    "#sum=0\n",
    "error=[]\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path,\"rt\") as f:\n",
    "            lines=f.readlines()\n",
    "            episod=[]\n",
    "            for x in lines:\n",
    "                try:\n",
    "                    #print(x.split(\" \")[4])\n",
    "                    episod.append(x.split(\" \")[4])\n",
    "                except:\n",
    "                    #sum+=1\n",
    "                    error.append(x)\n",
    "                    continue\n",
    "        collection.append(episod)\n",
    "        \n",
    "untokenized_collection=[]\n",
    "        \n",
    "for episod in range(len(collection)):\n",
    "    untokenized_collection.append(untokenize(collection[episod]))\n",
    "\n",
    "#print(sum)\n",
    "print (untokenized_collection[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer le tf-idf and prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After an unsuccessful visit to the high-IQ sperm bank, Dr. Leonard Hofstadter and Dr. Sheldon Cooper return home to find aspiring actress Penny is their new neighbor across the hall from their apartment. Sheldon thinks Leonard, who is immediately interested in her, is chasing a dream he will never catch. Leonard invites Penny to his and Sheldon's apartment for Indian food, where she asks to use their shower since hers is broken. While wrapped in a towel, she gets to meet their visiting friends Howard Wolowitz, a wannabe ladies' man who tries to hit on her, and Rajesh Koothrappali, who is unable to speak to her as he suffers from selective mutism in the presence of women. Leonard is so infatuated with Penny that, after helping her use their shower, he agrees to retrieve her TV from her ex-boyfriend Kurt. However, Kurt's physical superiority overwhelms Leonard's and Sheldon's combined IQ of 360, and they return without pants or TV. Penny, feeling bad, offers to take the guys out to dinner, initiating a friendship with them.\n"
     ]
    }
   ],
   "source": [
    "dir_path = './summary/'\n",
    "f_list = sorted(os.listdir(dir_path))\n",
    "file_list=[]\n",
    "file_list.append(f_list[0])\n",
    "file_list.extend(f_list[9:])\n",
    "file_list.extend(f_list[1:9])\n",
    "r_collection=[] #document collection\n",
    "r_error=[]\n",
    "p_error=[]\n",
    "#sum=0\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(dir_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path,\"rt\") as f:\n",
    "            lines=f.readlines()\n",
    "            episod=[]\n",
    "            for x in lines:\n",
    "                episod.append(x)\n",
    "        r_collection.append(episod)\n",
    "        \n",
    "r_untokenized_collection=[]\n",
    "        \n",
    "for episod in range(len(r_collection)):\n",
    "    try:\n",
    "        r_untokenized_collection.append(untokenize(r_collection[episod]))\n",
    "    except:\n",
    "        r_error.append(r_collection[episod])\n",
    "        p_error.append(episod)\n",
    "        continue\n",
    "\n",
    "\n",
    "print (r_untokenized_collection[0])\n",
    "#print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared! You can make your summarize now!\n"
     ]
    }
   ],
   "source": [
    "oral_words = ['oh', 'yes', 'no', 'hi', 'okay', 'uh', 'okay', 'bye', 'sorry', 'well', 'think','know','going','yeah'] #A compléter dans les tests\n",
    "stop_words = stopwords.words('english') + list(punctuation) + oral_words\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "tfidf.fit(untokenized_collection[episod] for episod in range(len(untokenized_collection)))\n",
    "print ('Model prepared! You can make your summarize now!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for changing the episod, just modify the index of untokenized_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits.\", u'I think we should be good neighbors, invite her over, make her feel welcome.', u'Who hasn\\'t seen this differential below \" here I sit broken hearted? \"', u'Well- Just sit somewhere else.', u\"Yeah, it's like regular boggle, but in Klingon.\", u\"Yes, it tells us that you participate in the mass cultural delusion that the Sun's apparent position relative to arbitrarily defined constellations and the time of your birth somehow effects your personality.\", u\"Okay, let's see, what else, oh, I'm a vegetarian, oh, except for fish, and the occasional steak, I love steak.\", u'You guys are really sweet.', u\"I'm not going to engage in hypotheticals here, I'm just trying to be a good neighbor.\", u'Hey, is there a trick to getting it to switch from tub to shower?', u\"It's French for good shower.\", u\"Why can't she get her own TV.\", u'Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building.', u'How the hell did you get in the building?', u\"Why don't you put some clothes on, I'll get my purse and dinner is on me, okay?\"]\n"
     ]
    }
   ],
   "source": [
    "most_frequent_words = get_frequent_word(tfidf, untokenized_collection[0])\n",
    "working_sentences, normal_sentences = make_sentence (untokenized_collection[0])\n",
    "num_sentences = 15\n",
    "out=get_output_sentence(most_frequent_words, working_sentences, normal_sentences,num_sentences)\n",
    "print (reorder_sentences(out, normal_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ngrams(n, text):\n",
    "    \"\"\"Calcualtes n-grams.\n",
    "    Args:\n",
    "      n: which n-grams to calculate\n",
    "      text: An array of tokens\n",
    "    Returns:\n",
    "      A set of n-grams\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    text_length = len(text)\n",
    "    max_index_ngram_start = text_length - n\n",
    "    for i in range(max_index_ngram_start + 1):\n",
    "        ngram_set.add(tuple(text[i:i + n]))\n",
    "    return ngram_set\n",
    "\n",
    "\n",
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
    "    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "#     return sentences.split(\" \")\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "    \"\"\"Calculates word n-grams for multiple sentences.\n",
    "    \"\"\"\n",
    "    assert len(sentences) > 0\n",
    "    assert n > 0\n",
    "\n",
    "    words = _split_into_words(sentences)\n",
    "    return _get_ngrams(n, words)\n",
    "\n",
    "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-N of two text collections of sentences.\n",
    "    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "    papers/rouge-working-note-v1.3.1.pdf\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentences: The sentences from the referene set\n",
    "      n: Size of ngram.  Defaults to 2.\n",
    "    Returns:\n",
    "      A tuple (f1, precision, recall) for ROUGE-N\n",
    "    Raises:\n",
    "      ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "#     print (evaluated_ngrams)\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    # Gets the overlapping ngrams between evaluated and reference\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"F1-score\": f1_score,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Recall': 0.6, u'F1-score': 0.5999999950000001, u'Precision': 0.6}\n",
      "{u'Recall': 0.5, u'F1-score': 0.4999999950000001, u'Precision': 0.5}\n"
     ]
    }
   ],
   "source": [
    "h1=\"he is a good man.\"\n",
    "r1=\"she is a good woman.\"\n",
    "h1=[h1]\n",
    "r1=[r1]\n",
    "r_1=rouge_n(h1,r1,1)\n",
    "r_2=rouge_n(h1,r1,2)\n",
    "print(r_1)\n",
    "print(r_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _len_lcs(x, y):\n",
    "    \"\"\"\n",
    "    Returns the length of the Longest Common Subsequence between sequences x\n",
    "    and y.\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: sequence of words\n",
    "      y: sequence of words\n",
    "    Returns\n",
    "      integer: Length of LCS between x and y\n",
    "    \"\"\"\n",
    "    table = _lcs(x, y)\n",
    "    n, m = len(x), len(y)\n",
    "    return table[n, m]\n",
    "\n",
    "\n",
    "def _lcs(x, y):\n",
    "    \"\"\"\n",
    "    Computes the length of the longest common subsequence (lcs) between two\n",
    "    strings. The implementation below uses a DP programming algorithm and runs\n",
    "    in O(nm) time where n = len(x) and m = len(y).\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: collection of words\n",
    "      y: collection of words\n",
    "    Returns:\n",
    "      Table of dictionary of coord and len lcs\n",
    "    \"\"\"\n",
    "    n, m = len(x), len(y)\n",
    "    table = dict()\n",
    "    for i in range(n + 1):\n",
    "        for j in range(m + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                table[i, j] = 0\n",
    "            elif x[i - 1] == y[j - 1]:\n",
    "                table[i, j] = table[i - 1, j - 1] + 1\n",
    "            else:\n",
    "                table[i, j] = max(table[i - 1, j], table[i, j - 1])\n",
    "    return table\n",
    "\n",
    "\n",
    "def _recon_lcs(x, y):\n",
    "    \"\"\"\n",
    "    Returns the Longest Subsequence between x and y.\n",
    "    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n",
    "    Args:\n",
    "      x: sequence of words\n",
    "      y: sequence of words\n",
    "    Returns:\n",
    "      sequence: LCS of x and y\n",
    "    \"\"\"\n",
    "    i, j = len(x), len(y)\n",
    "    table = _lcs(x, y)\n",
    "\n",
    "    def _recon(i, j):\n",
    "        \"\"\"private recon calculation\"\"\"\n",
    "        if i == 0 or j == 0:\n",
    "            return []\n",
    "        elif x[i - 1] == y[j - 1]:\n",
    "            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n",
    "        elif table[i - 1, j] > table[i, j - 1]:\n",
    "            return _recon(i - 1, j)\n",
    "        else:\n",
    "            return _recon(i, j - 1)\n",
    "\n",
    "    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n",
    "    return recon_tuple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _union_lcs(evaluated_sentences, reference_sentence, prev_union=None):\n",
    "    \"\"\"\n",
    "    Returns LCS_u(r_i, C) which is the LCS score of the union longest common\n",
    "    subsequence between reference sentence ri and candidate summary C.\n",
    "    For example:\n",
    "    if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8\n",
    "    and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1\n",
    "    is \"w1 w2\" and the longest common subsequence of r_i and c2 is \"w1 w3 w5\".\n",
    "    The union longest common subsequence of r_i, c1, and c2 is \"w1 w2 w3 w5\"\n",
    "    and LCS_u(r_i, C) = 4/5.\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentence: One of the sentences in the reference summaries\n",
    "    Returns:\n",
    "      float: LCS_u(r_i, C)\n",
    "    ValueError:\n",
    "      Raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if prev_union is None:\n",
    "        prev_union = set()\n",
    "\n",
    "    if len(evaluated_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    lcs_union = prev_union\n",
    "    prev_count = len(prev_union)\n",
    "    reference_words = _split_into_words([reference_sentence])\n",
    "\n",
    "    combined_lcs_length = 0\n",
    "    for eval_s in evaluated_sentences:\n",
    "        evaluated_words = _split_into_words([eval_s])\n",
    "        lcs = set(_recon_lcs(reference_words, evaluated_words))\n",
    "        combined_lcs_length += len(lcs)\n",
    "        lcs_union = lcs_union.union(lcs)\n",
    "\n",
    "    new_lcs_count = len(lcs_union) - prev_count\n",
    "    return new_lcs_count, lcs_union\n",
    "\n",
    "\n",
    "def rouge_l_summary_level(evaluated_sentences, reference_sentences):\n",
    "    \"\"\"\n",
    "    Computes ROUGE-L (summary level) of two text collections of sentences.\n",
    "    http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n",
    "    rouge-working-note-v1.3.1.pdf\n",
    "    Calculated according to:\n",
    "    R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n",
    "    P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n",
    "    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n",
    "    where:\n",
    "    SUM(i,u) = SUM from i through u\n",
    "    u = number of sentences in reference summary\n",
    "    C = Candidate summary made up of v sentences\n",
    "    m = number of words in reference summary\n",
    "    n = number of words in candidate summary\n",
    "    Args:\n",
    "      evaluated_sentences: The sentences that have been picked by the\n",
    "                           summarizer\n",
    "      reference_sentence: One of the sentences in the reference summaries\n",
    "    Returns:\n",
    "      A float: F_lcs\n",
    "    Raises:\n",
    "      ValueError: raises exception if a param has len <= 0\n",
    "    \"\"\"\n",
    "    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "        raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "    # total number of words in reference sentences\n",
    "    m = len(set(_split_into_words(reference_sentences)))\n",
    "\n",
    "    # total number of words in evaluated sentences\n",
    "    n = len(set(_split_into_words(evaluated_sentences)))\n",
    "\n",
    "    # print(\"m,n %d %d\" % (m, n))\n",
    "    union_lcs_sum_across_all_references = 0\n",
    "    union = set()\n",
    "    for ref_s in reference_sentences:\n",
    "        lcs_count, union = _union_lcs(evaluated_sentences,\n",
    "                                      ref_s,\n",
    "                                      prev_union=union)\n",
    "        union_lcs_sum_across_all_references += lcs_count\n",
    "\n",
    "    llcs = union_lcs_sum_across_all_references\n",
    "    r_lcs = llcs / m\n",
    "    p_lcs = llcs / n\n",
    "    beta = p_lcs / (r_lcs + 1e-12)\n",
    "    num = (1 + (beta**2)) * r_lcs * p_lcs\n",
    "    denom = r_lcs + ((beta**2) * p_lcs)\n",
    "    f_lcs = num / (denom + 1e-12)\n",
    "    return {\"Precision\": p_lcs, \"Recall\": r_lcs, \"F1-score\": f_lcs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Recall': 0.6, u'F1-score': 0.5999999999994999, u'Precision': 0.6}\n"
     ]
    }
   ],
   "source": [
    "r_l=rouge_l_summary_level(h1,r1)\n",
    "print(r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"If it's unobserved it will, however, if it's observed after it's left the plane but before it hits its target, it will not have gone through both slits.\", u'I think we should be good neighbors, invite her over, make her feel welcome.', u'Who hasn\\'t seen this differential below \" here I sit broken hearted? \"', u'Well- Just sit somewhere else.', u\"Yeah, it's like regular boggle, but in Klingon.\", u\"Yes, it tells us that you participate in the mass cultural delusion that the Sun's apparent position relative to arbitrarily defined constellations and the time of your birth somehow effects your personality.\", u\"Okay, let's see, what else, oh, I'm a vegetarian, oh, except for fish, and the occasional steak, I love steak.\", u'You guys are really sweet.', u\"I'm not going to engage in hypotheticals here, I'm just trying to be a good neighbor.\", u'Hey, is there a trick to getting it to switch from tub to shower?', u\"It's French for good shower.\", u\"Why can't she get her own TV.\", u'Come on, we have a combined IQ of 360, we should be able to figure out how to get into a stupid building.', u'How the hell did you get in the building?', u\"Why don't you put some clothes on, I'll get my purse and dinner is on me, okay?\"]\n",
      "[u'after an unsuccessful visit to the high-iq sperm bank, dr. leonard hofstadter and dr. sheldon cooper return home to find aspiring actress penny is their new neighbor across the hall from their apartment.', u'sheldon thinks leonard, who is immediately interested in her, is chasing a dream he will never catch.', u\"leonard invites penny to his and sheldon's apartment for indian food, where she asks to use their shower since hers is broken.\", u\"while wrapped in a towel, she gets to meet their visiting friends howard wolowitz, a wannabe ladies' man who tries to hit on her, and rajesh koothrappali, who is unable to speak to her as he suffers from selective mutism in the presence of women.\", u'leonard is so infatuated with penny that, after helping her use their shower, he agrees to retrieve her tv from her ex-boyfriend kurt.', u\"however, kurt's physical superiority overwhelms leonard's and sheldon's combined iq of 360, and they return without pants or tv.\", u'penny, feeling bad, offers to take the guys out to dinner, initiating a friendship with them.']\n"
     ]
    }
   ],
   "source": [
    "test=reorder_sentences(out,normal_sentences)\n",
    "print(test)\n",
    "\n",
    "reference_text=r_untokenized_collection[0]\n",
    "t1,t2=make_sentence(reference_text)\n",
    "\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"\"\n",
    "for i in t1:\n",
    "    s+=i\n",
    "s=[s]\n",
    "\n",
    "s1=\"\"\n",
    "for i in test:\n",
    "    s1+=i\n",
    "\n",
    "s1=[s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hyp, ref in zip(s, s1):\n",
    "        hyp = [\" \".join(_.split()) for _ in hyp.split(\".\") if len(_) > 0]\n",
    "        ref = [\" \".join(_.split()) for _ in ref.split(\".\") if len(_) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Recall': 0.14193548387096774, u'F1-score': 0.1605839366921521, u'Precision': 0.18487394957983194}\n",
      "{u'Recall': 0.009615384615384616, u'F1-score': 0.010554084757418382, u'Precision': 0.011695906432748537}\n",
      "{u'Recall': 0.13548387096774195, u'F1-score': 0.14825308918355073, u'Precision': 0.17647058823529413}\n"
     ]
    }
   ],
   "source": [
    "print(rouge_n(hyp,ref,1))\n",
    "print(rouge_n(hyp,ref,2))\n",
    "print(rouge_l_summary_level(hyp,ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "呜啦啦啦火车笛，做projet很忙滴。。大家唱起来！俗话说的好，彩蛋最后找，要是没找到。。。。。。。。。。。。。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "说明你小子根本没仔细看劳资辛辛苦苦写的代码！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
